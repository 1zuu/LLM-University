{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/1zuu/miniforge3/envs/llamaindex/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading readme: 100%|██████████| 381/381 [00:00<00:00, 1.63MB/s]\n",
      "/Users/1zuu/miniforge3/envs/llamaindex/lib/python3.9/site-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n",
      "Downloading data: 100%|██████████| 107k/107k [00:00<00:00, 175kB/s]\n",
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00,  1.32it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 397.00it/s]\n",
      "Generating train split: 400 examples [00:00, 11942.44 examples/s]\n",
      "vocab.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 1.63MB/s]\n",
      "merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 6.01MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 2.37MB/s]\n",
      "config.json: 100%|██████████| 762/762 [00:00<00:00, 3.27MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length set to:  512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datapipe import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = 0\n",
    "batch_size = 8\n",
    "num_epochs = 10\n",
    "model_name = 'distilgpt2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|██████████| 353M/353M [01:14<00:00, 4.74MB/s] \n",
      "generation_config.json: 100%|██████████| 124/124 [00:00<00:00, 72.8kB/s]\n"
     ]
    }
   ],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('distilgpt2').to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = tokenizer.pad_token_id)\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-4)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init a results dataframe\n",
    "results = pd.DataFrame(columns=['epoch', 'transformer', 'batch_size', 'gpu',\n",
    "                                'training_loss', 'validation_loss', 'epoch_duration_sec'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/10 Batch Size: 8, Transformer: distilgpt2: 100%|██████████| 40/40 [00:17<00:00,  2.34it/s, Training Loss=10.3] \n",
      "Validation Epoch 1/10: 100%|██████████| 10/10 [00:00<00:00, 12.47it/s, Validation Loss=10] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Validation Loss: 9.955349922180176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/10 Batch Size: 8, Transformer: distilgpt2: 100%|██████████| 40/40 [00:13<00:00,  3.06it/s, Training Loss=3.87]\n",
      "Validation Epoch 2/10: 100%|██████████| 10/10 [00:00<00:00, 12.38it/s, Validation Loss=4.99]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Validation Loss: 4.388497352600098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/10 Batch Size: 8, Transformer: distilgpt2: 100%|██████████| 40/40 [00:13<00:00,  3.06it/s, Training Loss=9.45]\n",
      "Validation Epoch 3/10: 100%|██████████| 10/10 [00:00<00:00, 12.55it/s, Validation Loss=9.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Validation Loss: 8.26036262512207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/10 Batch Size: 8, Transformer: distilgpt2: 100%|██████████| 40/40 [00:13<00:00,  2.97it/s, Training Loss=7.46]\n",
      "Validation Epoch 4/10: 100%|██████████| 10/10 [00:00<00:00, 12.42it/s, Validation Loss=7.86]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Validation Loss: 6.821663856506348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/10 Batch Size: 8, Transformer: distilgpt2: 100%|██████████| 40/40 [00:13<00:00,  3.01it/s, Training Loss=6.29]\n",
      "Validation Epoch 5/10: 100%|██████████| 10/10 [00:00<00:00, 12.40it/s, Validation Loss=10.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Validation Loss: 8.952461242675781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/10 Batch Size: 8, Transformer: distilgpt2: 100%|██████████| 40/40 [00:13<00:00,  3.04it/s, Training Loss=11.5]\n",
      "Validation Epoch 6/10: 100%|██████████| 10/10 [00:00<00:00, 11.01it/s, Validation Loss=10.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Validation Loss: 9.536514282226562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7/10 Batch Size: 8, Transformer: distilgpt2: 100%|██████████| 40/40 [00:13<00:00,  2.94it/s, Training Loss=9.18]\n",
      "Validation Epoch 7/10: 100%|██████████| 10/10 [00:00<00:00, 10.52it/s, Validation Loss=11.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Validation Loss: 10.26335334777832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8/10 Batch Size: 8, Transformer: distilgpt2: 100%|██████████| 40/40 [00:13<00:00,  2.86it/s, Training Loss=12.9]\n",
      "Validation Epoch 8/10: 100%|██████████| 10/10 [00:00<00:00, 11.04it/s, Validation Loss=12.1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Validation Loss: 10.555124282836914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9/10 Batch Size: 8, Transformer: distilgpt2: 100%|██████████| 40/40 [00:13<00:00,  2.91it/s, Training Loss=7.19]\n",
      "Validation Epoch 9/10: 100%|██████████| 10/10 [00:00<00:00, 10.93it/s, Validation Loss=11.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Validation Loss: 10.212566375732422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10/10 Batch Size: 8, Transformer: distilgpt2: 100%|██████████| 40/40 [00:13<00:00,  2.96it/s, Training Loss=8.84]\n",
      "Validation Epoch 10/10: 100%|██████████| 10/10 [00:00<00:00, 10.69it/s, Validation Loss=10.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Validation Loss: 9.4764404296875\n"
     ]
    }
   ],
   "source": [
    "# The training loop\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()  # Start the timer for the epoch\n",
    "\n",
    "    # Training\n",
    "    ## This line tells the model we're in 'learning mode'\n",
    "    model.train()\n",
    "    epoch_training_loss = 0\n",
    "    train_iterator = tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs} Batch Size: {batch_size}, Transformer: {model_name}\")\n",
    "    for batch in train_iterator:\n",
    "        optimizer.zero_grad()\n",
    "        inputs = batch['input_ids'].squeeze(1).to(device)\n",
    "        targets = inputs.clone()\n",
    "        outputs = model(input_ids=inputs, labels=targets)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_iterator.set_postfix({'Training Loss': loss.item()})\n",
    "        epoch_training_loss += loss.item()\n",
    "    avg_epoch_training_loss = epoch_training_loss / len(train_iterator)\n",
    "\n",
    "    # Validation\n",
    "    ## This line below tells the model to 'stop learning'\n",
    "    model.eval()\n",
    "    epoch_validation_loss = 0\n",
    "    total_loss = 0\n",
    "    valid_iterator = tqdm(valid_loader, desc=f\"Validation Epoch {epoch+1}/{num_epochs}\")\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_iterator:\n",
    "            inputs = batch['input_ids'].squeeze(1).to(device)\n",
    "            targets = inputs.clone()\n",
    "            outputs = model(input_ids=inputs, labels=targets)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss\n",
    "            valid_iterator.set_postfix({'Validation Loss': loss.item()})\n",
    "            epoch_validation_loss += loss.item()\n",
    "\n",
    "    avg_epoch_validation_loss = epoch_validation_loss / len(valid_loader)\n",
    "\n",
    "    end_time = time.time()  # End the timer for the epoch\n",
    "    epoch_duration_sec = end_time - start_time  # Calculate the duration in seconds\n",
    "\n",
    "    new_row = {'transformer': model_name,\n",
    "               'batch_size': batch_size,\n",
    "               'gpu': gpu,\n",
    "               'epoch': epoch+1,\n",
    "               'training_loss': avg_epoch_training_loss,\n",
    "               'validation_loss': avg_epoch_validation_loss,\n",
    "               'epoch_duration_sec': epoch_duration_sec}  # Add epoch_duration to the dataframe\n",
    "\n",
    "    results.loc[len(results)] = new_row\n",
    "    print(f\"Epoch: {epoch+1}, Validation Loss: {total_loss/len(valid_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_str = \"Kidney Failure\"\n",
    "input_ids = tokenizer.encode(input_str, return_tensors='pt').to(device)\n",
    "\n",
    "output = model.generate(\n",
    "                        input_ids,\n",
    "                        max_length=20,\n",
    "                        num_return_sequences=1,\n",
    "                        do_sample=True,\n",
    "                        top_k=8,\n",
    "                        top_p=0.95,\n",
    "                        temperature=0.5,\n",
    "                        repetition_penalty=1.2\n",
    "                        )\n",
    "\n",
    "decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(decoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'SmallMedLM.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaindex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
