{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Training Pipeline for Alpaca 7B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](vis/image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](vis/image-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### we use together ai to load models for convienience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, yaml, together, logging\n",
    "from langchain.llms.base import LLM\n",
    "from pydantic import Extra, Field, root_validator\n",
    "from typing import Any, Dict, List, Mapping, Optional\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain.llms.utils import enforce_stop_tokens\n",
    "from langchain.utils import get_from_dict_or_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cadentials.yaml') as f:\n",
    "    credentials = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "os.environ['TOGETHER_AI_API'] = credentials['TOGETHER_AI_API']\n",
    "together.api_key = os.environ['TOGETHER_AI_API']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## Below are the models available ########\n",
      "Austism/chronos-hermes-13b\n",
      "EleutherAI/pythia-12b-v0\n",
      "EleutherAI/pythia-1b-v0\n",
      "EleutherAI/pythia-2.8b-v0\n",
      "EleutherAI/pythia-6.9b\n",
      "Gryphe/MythoMax-L2-13b\n",
      "HuggingFaceH4/starchat-alpha\n",
      "NousResearch/Nous-Hermes-13b\n",
      "NousResearch/Nous-Hermes-Llama2-13b\n",
      "NumbersStation/nsql-llama-2-7B\n",
      "OpenAssistant/llama2-70b-oasst-sft-v10\n",
      "OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\n",
      "OpenAssistant/stablelm-7b-sft-v7-epoch-3\n",
      "Phind/Phind-CodeLlama-34B-Python-v1\n",
      "Phind/Phind-CodeLlama-34B-v2\n",
      "SG161222/Realistic_Vision_V3.0_VAE\n",
      "WizardLM/WizardCoder-15B-V1.0\n",
      "WizardLM/WizardCoder-Python-34B-V1.0\n",
      "WizardLM/WizardLM-70B-V1.0\n",
      "bigcode/starcoder\n",
      "databricks/dolly-v2-12b\n",
      "databricks/dolly-v2-3b\n",
      "databricks/dolly-v2-7b\n",
      "defog/sqlcoder\n",
      "garage-bAInd/Platypus2-70B-instruct\n",
      "huggyllama/llama-13b\n",
      "huggyllama/llama-30b\n",
      "huggyllama/llama-65b\n",
      "huggyllama/llama-7b\n",
      "lmsys/fastchat-t5-3b-v1.0\n",
      "lmsys/vicuna-13b-v1.3\n",
      "lmsys/vicuna-13b-v1.5-16k\n",
      "lmsys/vicuna-13b-v1.5\n",
      "lmsys/vicuna-7b-v1.3\n",
      "mistralai/Mistral-7B-Instruct-v0.1\n",
      "mistralai/Mistral-7B-v0.1\n",
      "prompthero/openjourney\n",
      "runwayml/stable-diffusion-v1-5\n",
      "stabilityai/stable-diffusion-2-1\n",
      "stabilityai/stable-diffusion-xl-base-1.0\n",
      "togethercomputer/CodeLlama-13b-Instruct\n",
      "togethercomputer/CodeLlama-13b-Python\n",
      "togethercomputer/CodeLlama-13b\n",
      "togethercomputer/CodeLlama-34b-Instruct\n",
      "togethercomputer/CodeLlama-34b-Python\n",
      "togethercomputer/CodeLlama-34b\n",
      "togethercomputer/CodeLlama-7b-Instruct\n",
      "togethercomputer/CodeLlama-7b-Python\n",
      "togethercomputer/CodeLlama-7b\n",
      "togethercomputer/GPT-JT-6B-v1\n",
      "togethercomputer/GPT-JT-Moderation-6B\n",
      "togethercomputer/GPT-NeoXT-Chat-Base-20B\n",
      "togethercomputer/Koala-13B\n",
      "togethercomputer/LLaMA-2-7B-32K\n",
      "togethercomputer/Llama-2-7B-32K-Instruct\n",
      "togethercomputer/Pythia-Chat-Base-7B-v0.16\n",
      "togethercomputer/Qwen-7B-Chat\n",
      "togethercomputer/Qwen-7B\n",
      "togethercomputer/RedPajama-INCITE-7B-Base\n",
      "togethercomputer/RedPajama-INCITE-7B-Chat\n",
      "togethercomputer/RedPajama-INCITE-7B-Instruct\n",
      "togethercomputer/RedPajama-INCITE-Base-3B-v1\n",
      "togethercomputer/RedPajama-INCITE-Chat-3B-v1\n",
      "togethercomputer/RedPajama-INCITE-Instruct-3B-v1\n",
      "togethercomputer/alpaca-7b\n",
      "togethercomputer/codegen2-16B\n",
      "togethercomputer/codegen2-7B\n",
      "togethercomputer/falcon-40b-instruct\n",
      "togethercomputer/falcon-40b\n",
      "togethercomputer/falcon-7b-instruct\n",
      "togethercomputer/falcon-7b\n",
      "togethercomputer/guanaco-13b\n",
      "togethercomputer/guanaco-33b\n",
      "togethercomputer/guanaco-65b\n",
      "togethercomputer/guanaco-7b\n",
      "togethercomputer/llama-2-13b-chat\n",
      "togethercomputer/llama-2-13b\n",
      "togethercomputer/llama-2-70b-chat\n",
      "togethercomputer/llama-2-70b\n",
      "togethercomputer/llama-2-7b-chat\n",
      "togethercomputer/llama-2-7b\n",
      "togethercomputer/mpt-30b-chat\n",
      "togethercomputer/mpt-30b-instruct\n",
      "togethercomputer/mpt-30b\n",
      "togethercomputer/mpt-7b-chat\n",
      "togethercomputer/mpt-7b\n",
      "togethercomputer/replit-code-v1-3b\n",
      "upstage/SOLAR-0-70b-16bit\n",
      "wavymulder/Analog-Diffusion\n"
     ]
    }
   ],
   "source": [
    "models = together.Models.list()\n",
    "print(\"######## Below are the models available ########\")\n",
    "for model in models:\n",
    "    print(model['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'success': True,\n",
       " 'value': 'e196cc334bf5e3a6013fc38a42f7c1764a8fd41ace83bf0a42f407b49090d616'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_card = 'togethercomputer/alpaca-7b'\n",
    "together.Models.start(model_card)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TogetherLLM(LLM):\n",
    "    \"\"\"Together large language models.\"\"\"\n",
    "\n",
    "    model: str = model_card\n",
    "    \"\"\"model endpoint to use\"\"\"\n",
    "\n",
    "    together_api_key: str = os.environ[\"TOGETHER_AI_API\"]\n",
    "    \"\"\"Together API key\"\"\"\n",
    "\n",
    "    temperature: float = 0.7\n",
    "    \"\"\"What sampling temperature to use.\"\"\"\n",
    "\n",
    "    max_tokens: int = 512\n",
    "    \"\"\"The maximum number of tokens to generate in the completion.\"\"\"\n",
    "\n",
    "    class Config:\n",
    "        extra = Extra.forbid\n",
    "\n",
    "    @root_validator()\n",
    "    def validate_environment(cls, values: Dict) -> Dict:\n",
    "        \"\"\"Validate that the API key is set.\"\"\"\n",
    "        api_key = get_from_dict_or_env(\n",
    "                                        values, \n",
    "                                        \"together_ai_api\", \n",
    "                                        \"TOGETHER_AI_API\"\n",
    "                                        )\n",
    "        values[\"together_ai_api\"] = api_key\n",
    "        return values\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Return type of LLM.\"\"\"\n",
    "        return \"together\"\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        \"\"\"Call to Together endpoint.\"\"\"\n",
    "        together.api_key = self.together_api_key\n",
    "        output = together.Complete.create(prompt,\n",
    "                                          model=self.model,\n",
    "                                          max_tokens=self.max_tokens,\n",
    "                                          temperature=self.temperature,\n",
    "                                          )\n",
    "        text = output['output']['choices'][0]['text']\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_llm = TogetherLLM(\n",
    "                    model=model_card,\n",
    "                    temperature=0.1,\n",
    "                    max_tokens=512\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(__main__.TogetherLLM, 'togethercomputer/alpaca-7b', 0.1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_llm), test_llm.model, test_llm.temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Alpacas are a species of South American camelid, related to llamas and vicuñas. They are smaller than llamas, and have a finer fleece. Alpacas are shorn once a year, and the fleece is usually white, although other colours are available. Alpacas are also more docile than llamas, and make better pets.\n",
      "Lamas are a species of South American camelid, related to alpacas and vicuñas. They are larger than alpacas, and have a coarser fleece. Lamas are shorn twice a year, and the fleece is usually brown or grey. Lamas are more hardy than alpacas, and are often used as pack animals.</s>\n"
     ]
    }
   ],
   "source": [
    "res = test_llm(\"What are Alpacas and how are they different to Lamas?\")\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
